{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6ik0/de8aaxGJI1PUku6a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaredKeithAveritt/AI_methods_in_advanced_materials_research/blob/main/pytorch_non_linear_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework: train a Nonlinear Classifier\n",
        "\n",
        "[NonlinearClassifier as we introduced at the end of week 4](https://github.com/JaredKeithAveritt/AI_methods_in_advanced_materials_research/blob/main/Week_4/01_pytorch_mnist.ipynb).\n",
        "\n",
        "\n",
        "Do the following to train the `NonlinearClassifier` and evaluate its performance on test data, using the following steps:\n",
        "\n",
        "1. **Initialize the Nonlinear Classifier Model.**\n",
        "2. **Create Data Loaders for Training and Test Data.**\n",
        "3. **Define a Loss Function and an Optimizer.**\n",
        "4. **Train the Model.**\n",
        "5. **Evaluate the Model on Test Data.**\n",
        "\n",
        "### Experimenting with Improvements\n",
        "\n",
        "To improve the model, consider experimenting with:\n",
        "- **Increasing Model Complexity**: Adding more layers or increasing the number of neurons in existing layers.\n",
        "- **Changing Activation Functions**: Experimenting with different activation functions like LeakyReLU or ELU.\n",
        "- **Adjusting the Learning Rate**: Tuning the learning rate or using learning rate schedulers.\n",
        "- **Using Different Optimizers**: Trying out optimizers like Adam or RMSprop instead of SGD.\n",
        "- **Implementing Regularization**: Adding dropout layers or using L2 regularization to prevent overfitting.\n",
        "\n",
        "Remember, compare models using training and validation data. The test data should only be used as a final check to assess generalization capability.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "krVP258JAzHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 -- Dataset Loading and Preprocessing of Input Features\n",
        "\n",
        "This is the same as from the notebook from week 4."
      ],
      "metadata": {
        "id": "UWYnb8afBMU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "#This is a magic command (can only use 1 per code block, and must be the first line) for Jupyter notebooks and IPython environments. It ensures that the output of plotting commands is displayed inline within frontends like the Jupyter notebook, directly below the code cell that produced it. The plots will be stored in the notebook document.\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "metadata": {
        "id": "EQ1KW6TvCA2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load and transform the training data\n",
        "training_data = torchvision.datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor()\n",
        ")\n",
        "\n",
        "# Load and transform the test data\n",
        "# Similar to the training data loader but with 'train=False' to specify that we want to\n",
        "# load the test (or validation) portion of the MNIST dataset. This data is used to evaluate\n",
        "# the model's performance on unseen data, providing an estimate of its generalization ability.\n",
        "test_data = torchvision.datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor()\n",
        ")\n",
        "\n",
        "total_size = len(training_data)  # Total size of the dataset\n",
        "\n",
        "# Calculate split sizes\n",
        "train_size = int(total_size * 0.8)  # 80% of the dataset for training\n",
        "validation_size = total_size - train_size  # Remaining 20% for validation\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "training_data, validation_data = torch.utils.data.random_split(\n",
        "    training_data, [train_size, validation_size],\n",
        "    generator=torch.Generator().manual_seed(55)  # Ensure reproducibility\n",
        ")\n",
        "\n",
        "# 'training_data' now contains the training subset,\n",
        "# 'validation_data' contains the validation subset.\n",
        "\n",
        "# Print the size of training, validation, and test datasets\n",
        "print('MNIST data loaded: train:', len(training_data), 'examples,',\n",
        "      'validation:', len(validation_data), 'examples,',\n",
        "      'test:', len(test_data), 'examples')\n",
        "\n",
        "# Print the shape of the input data by accessing the first example in the training dataset\n",
        "# Note: training_data[0][0] accesses the first image tensor, and .shape retrieves its dimensions\n",
        "print('Input shape:', training_data[0][0].shape)\n",
        "\n",
        "pltsize = 1\n",
        "# Initialize figure with dimensions proportional to the number of images\n",
        "plt.figure(figsize=(10*pltsize, pltsize))\n",
        "\n",
        "# Display the first 10 images from the training dataset\n",
        "for i in range(10):\n",
        "    plt.subplot(1, 10, i+1)  # Prepare subplot for the ith image\n",
        "    plt.axis('off')  # Hide the axis for a cleaner look\n",
        "    # Display the image, reshaping it to 28x28 pixels, in grayscale\n",
        "    plt.imshow(numpy.reshape(training_data[i][0], (28, 28)), cmap=\"gray\")\n",
        "    # Add a title with the class of the digit\n",
        "    plt.title('Class: '+str(training_data[i][1]))"
      ],
      "metadata": {
        "id": "tPK-OJB6BIny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Step 1: Define Classifier and initialize it\n",
        "I already did this for you."
      ],
      "metadata": {
        "id": "VpNg4uXuBrwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NonlinearClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NonlinearClassifier, self).__init__()\n",
        "        # Flatten the input image to a vector\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Define a stack of layers: linear transformations followed by ReLU activations\n",
        "        self.layers_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 50),  # First layer with 784 inputs and 50 outputs\n",
        "            nn.ReLU(),  # Nonlinear activation function\n",
        "            # nn.Dropout(0.2),  # Optional dropout for regularization (commented out)\n",
        "            nn.Linear(50, 50),  # Second layer, from 50 to 50 neurons\n",
        "            nn.ReLU(),  # Another ReLU activation\n",
        "            # nn.Dropout(0.2),  # Another optional dropout layer (commented out)\n",
        "            nn.Linear(50, 10)   # Final layer that outputs to the 10 classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten and then pass the data through the layers stack\n",
        "        x = self.flatten(x)\n",
        "        x = self.layers_stack(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the Nonlinear Classifier Model\n",
        "nonlinear_model = NonlinearClassifier()"
      ],
      "metadata": {
        "id": "yEs4Xf8JAtys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Step 2: Create Data Loaders for Training and Test Data\n",
        "Assuming `training_data` and `test_data` have already been defined as in previous examples.\n",
        "\n",
        "Hint: [see the documentation on PyTorch site](https://pytorch.org/docs/stable/data.html)"
      ],
      "metadata": {
        "id": "0jSyhRWzGKOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Code goes here\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "93JMQU-_Gb7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Step 3: Define a Loss Function and an Optimizer\n"
      ],
      "metadata": {
        "id": "DENfez1QGjUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code goes here\n"
      ],
      "metadata": {
        "id": "z9ql1B3bGkGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Step 4: Train your model\n",
        "\n",
        "Hint: A training step is comprised of:\n",
        "- A forward pass: the input is passed through the network\n",
        "- Backpropagation: A backward pass to compute the gradient $\\frac{\\partial J}{\\partial \\mathbf{W}}$ of the loss function with respect to the parameters of the network.\n",
        "- Weight updates $\\mathbf{W} = \\mathbf{W} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W}} $ where $\\alpha$ is the learning rate."
      ],
      "metadata": {
        "id": "OW64tUGJDAd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code goes here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YeCqWtfyBqlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the model for a certain number of epochs:\n",
        "\n"
      ],
      "metadata": {
        "id": "WC8Z6KMAG2Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code goes here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XqcVv35UG2MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Step 5: Evaluate the Model on Test Data\n",
        "Now, let's evaluate the model's performance on the test data:"
      ],
      "metadata": {
        "id": "92EobMN_HMyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code goes here\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BcIJhDcPD4sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXTRA: Experimenting with Improvements\n",
        "\n",
        "To improve the model, consider experimenting with:\n",
        "- **Increasing Model Complexity**: Adding more layers or increasing the number of neurons in existing layers.\n",
        "- **Changing Activation Functions**: Experimenting with different activation functions like LeakyReLU or ELU.\n",
        "- **Adjusting the Learning Rate**: Tuning the learning rate or using learning rate schedulers.\n",
        "- **Using Different Optimizers**: Trying out optimizers like Adam or RMSprop instead of SGD.\n",
        "- **Implementing Regularization**: Adding dropout layers or using L2 regularization to prevent overfitting."
      ],
      "metadata": {
        "id": "Kvjjbn_nHUZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code goes here\n"
      ],
      "metadata": {
        "id": "UnOfOq5VHSop"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}